{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import arrow\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of notebook\n",
    "\n",
    "Hi Evie, \n",
    "\n",
    "I'm writing you some code to extract data of interest from the 4 years of our hindcast. You would like timeseries of pH and $\\Omega_A$ for the whole watercolumn for the following predefined places:\n",
    "\n",
    "    list_places = ['Deep Bay', 'Southern Baynes', 'Northern Baynes', \n",
    "    'Fanny Bay', 'Maple Bay', 'Salt Spring', 'Nanoose Bay', 'Lasqueti Island', \n",
    "    'Main SoG','Cortes/Marina', 'Lund/Desolation Sound', 'Mouth of Okeover']\n",
    "\n",
    "As a review, our model outputs carbonate chemistry in the form of DIC and TA, in very large .nc files [(here is more info on .nc files)](https://www.unidata.ucar.edu/software/netcdf/). One day's worth is around a gigabyte. Biologically relevant variables such as pH and $\\Omega_A$ are then backcalculated from these (for this, you also need T and S). In this notebook (A), I'm going to show you code for extracting those data (DIC, TA, S, T) from our model output for all the places you want for a given year and saving it a different , much smaller, file - one file per day. In notebook B, I'll consolidate these daily small files into one bigger file. In notebook C, I'll then calculate pH and Omega and plot them. I've done my best to comment my code using #TJSJ (from my initials TJÅ J) to show what I'm doing. \n",
    "\n",
    "This code shows how to extract the relevant data for all stations for a given day and save it in a .nc file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TJSJ - first I make a dictionary of your places with lat, lon, and model coordinates. \n",
    "    \n",
    "    Original dictionary by E.Olson (I think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PLACES = {\n",
    "'Deep Bay': {\n",
    "\n",
    "# deg E, deg N\n",
    "\n",
    "'lon lat': (-124.7392, 49.4606),\n",
    "\n",
    "# indices of nearest NEMO model grid point\n",
    "\n",
    "# j is the latitude (y) direction, i is the longitude (x) direction\n",
    "\n",
    "'NEMO grid ji': (599, 126),\n",
    "\n",
    "},\n",
    "\n",
    "'Southern Baynes': {\n",
    "\n",
    "'lon lat': (-124.7457, 49.4760),\n",
    "\n",
    "'NEMO grid ji': (602, 127),\n",
    "\n",
    "},\n",
    "\n",
    "'Northern Baynes': {\n",
    "\n",
    "'lon lat': (-124.8924, 49.6492),\n",
    "\n",
    "'NEMO grid ji': (646, 127),\n",
    "\n",
    "},\n",
    "\n",
    "'Fanny Bay': {\n",
    "\n",
    "'lon lat': (-124.8227, 49.5086),\n",
    "\n",
    "'NEMO grid ji': (614, 120),\n",
    "\n",
    "},\n",
    "\n",
    "'Maple Bay': {\n",
    "\n",
    "'lon lat': (-123.5947, 48.8140),\n",
    "\n",
    "'NEMO grid ji': (392, 213),\n",
    "\n",
    "},\n",
    "\n",
    "'Salt Spring': {\n",
    "\n",
    "'lon lat': (-123.5513, 48.7993),\n",
    "\n",
    "'NEMO grid ji': (386, 218),\n",
    "\n",
    "},\n",
    "\n",
    "'Nanoose Bay': {\n",
    "\n",
    "'lon lat': (-124.1359, 49.2609),\n",
    "\n",
    "'NEMO grid ji': (517, 190),\n",
    "\n",
    "},\n",
    "\n",
    "'Lasqueti Island': {\n",
    "\n",
    "# deg E, deg N\n",
    "\n",
    "'lon lat': (-124.3384, 49.5442),\n",
    "\n",
    "'NEMO grid ji': (586, 195),\n",
    "\n",
    "},\n",
    "\n",
    "'Main SoG': {\n",
    "\n",
    "'lon lat': (-123.5832, 49.1177),\n",
    "\n",
    "'NEMO grid ji': (450, 253),\n",
    "\n",
    "},\n",
    "\n",
    "'Cortes/Marina': {\n",
    "\n",
    "'lon lat': (-125.0194, 50.0418),\n",
    "\n",
    "'NEMO grid ji': (732, 157),\n",
    "\n",
    "},\n",
    "\n",
    "'Lund/Desolation Sound': {\n",
    "\n",
    "'lon lat': (-124.7666, 49.9804),\n",
    "\n",
    "'NEMO grid ji': (702, 187),\n",
    "\n",
    "},\n",
    "\n",
    "'Mouth of Okeover': {\n",
    "\n",
    "'lon lat': (-124.8174, 50.0805),\n",
    "\n",
    "'NEMO grid ji': (726, 192),\n",
    "\n",
    "},\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TJSJ I make a list of place names, for calling the dictionary from\n",
    "    \n",
    "    then I test that I can access the data from the dictionary \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place is Deep Bay. The j,i coords are (599, 126)\n",
      "Place is Southern Baynes. The j,i coords are (602, 127)\n",
      "Place is Northern Baynes. The j,i coords are (646, 127)\n",
      "Place is Fanny Bay. The j,i coords are (614, 120)\n",
      "Place is Maple Bay. The j,i coords are (392, 213)\n",
      "Place is Salt Spring. The j,i coords are (386, 218)\n",
      "Place is Nanoose Bay. The j,i coords are (517, 190)\n",
      "Place is Lasqueti Island. The j,i coords are (586, 195)\n",
      "Place is Main SoG. The j,i coords are (450, 253)\n",
      "Place is Cortes/Marina. The j,i coords are (732, 157)\n",
      "Place is Lund/Desolation Sound. The j,i coords are (702, 187)\n",
      "Place is Mouth of Okeover. The j,i coords are (726, 192)\n"
     ]
    }
   ],
   "source": [
    "list_places = ['Deep Bay', 'Southern Baynes', 'Northern Baynes', \\\n",
    "               'Fanny Bay', 'Maple Bay', 'Salt Spring', 'Nanoose Bay',\\\n",
    "               'Lasqueti Island', 'Main SoG', 'Cortes/Marina', \\\n",
    "               'Lund/Desolation Sound', 'Mouth of Okeover']\n",
    "\n",
    "for i in range(0,len(list_places)):\n",
    "    t_place = (list_places[i])\n",
    "    print('Place is '+t_place +\\\n",
    "          '. The j,i coords are '+str(PLACES[t_place]['NEMO grid ji']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TJSJ Write a function that returns a list of paths to nc files for given days in a year. \n",
    "\n",
    "    To do that I basically define my start date and end date (see below) \n",
    "    and put all the dates between them in different formats using the arrow\n",
    "    function, then string together a string name based on what kind of data\n",
    "    I want (carbon or Temp/Sal). By convention that Susan made up, we save\n",
    "    the data in two filename -suffixes\n",
    "    \n",
    "        grid_T, (short for tracers on the T grid) which contains T and S \n",
    "        carp_T, (short for carbon plus) which contains DIC, TA, and other things\n",
    "\n",
    "    in the verbose option, the function spits out the dateformats \n",
    "    and what filename it's found for each date\n",
    "    below I test it for two days to make sure it's doing what I think it is \n",
    "    and print the resulting array of ncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable type is carp\n",
      "\n",
      "DATE IS: 2016-01-01T00:00:00+00:00\n",
      "Arrow ddmmmyy format is: 01jan16\n",
      "Arrow ymd format is: 20160101\n",
      "file is: /results/SalishSea/hindcast.201905/01jan16/SalishSea_1h_20160101_20160101_carp_T.nc\n",
      "\n",
      "DATE IS: 2016-01-02T00:00:00+00:00\n",
      "Arrow ddmmmyy format is: 02jan16\n",
      "Arrow ymd format is: 20160102\n",
      "file is: /results/SalishSea/hindcast.201905/02jan16/SalishSea_1h_20160102_20160102_carp_T.nc\n",
      "\n",
      "['/results/SalishSea/hindcast.201905/01jan16/SalishSea_1h_20160101_20160101_carp_T.nc', '/results/SalishSea/hindcast.201905/02jan16/SalishSea_1h_20160102_20160102_carp_T.nc']\n"
     ]
    }
   ],
   "source": [
    "start = '2016-01-01'\n",
    "end = '2016-01-02'\n",
    "verbose = True\n",
    "ftype = 'carp'\n",
    "\n",
    "def make_nclist(start,end,ftype,verbose):\n",
    "\n",
    "    print('Variable type is '+ftype)\n",
    "    print('')\n",
    "    nc_array = []\n",
    "    start_run = arrow.get(start)\n",
    "    end_run = arrow.get(end)\n",
    "    \n",
    "    \n",
    "    arrow_array = []\n",
    "    for r in arrow.Arrow.span_range('day', start_run, end_run):\n",
    "        arrow_array.append(r)\n",
    "\n",
    "    dayslen = len(arrow_array)\n",
    "    \n",
    "    for i in range(0,dayslen):\n",
    "        tdate = arrow_array[i][0]\n",
    "        if verbose == True:\n",
    "            print('DATE IS: ' +str(tdate))\n",
    "        ddmmmyy = tdate.format('DDMMMYY').lower()\n",
    "        if verbose == True:\n",
    "            print('Arrow ddmmmyy format is: '+ str(ddmmmyy))\n",
    "        ymd = tdate.format('YYYYMMDD')\n",
    "        if verbose == True:\n",
    "            print('Arrow ymd format is: '+ str(ymd))\n",
    "            \n",
    "        t_nc = '/results/SalishSea/hindcast.201905/' \\\n",
    "        + str(ddmmmyy)+ '/SalishSea_*' + str(ymd) + '_' + str(ymd) + '_' + ftype + '_T.nc'\n",
    "        t_ncname = glob.glob(t_nc)\n",
    "        if verbose == True:\n",
    "            print('file is: ' + t_ncname[0])\n",
    "            print('')\n",
    "        nc_array.append(t_ncname[0])\n",
    "        \n",
    "    return nc_array\n",
    "\n",
    "nc_array = make_nclist(start,end,ftype,verbose)\n",
    "\n",
    "print(nc_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TJSJ here I'm extracting the filenames for the year 2013 \n",
    "\n",
    "    for both the carbon (carp) and T,S (grid) filenames.\n",
    "    verbose set to False to avoid it vomiting output at me. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable type is carp\n",
      "\n",
      "Variable type is grid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = '2013-01-01'\n",
    "end = '2013-12-31'\n",
    "verbose = False\n",
    "ftype = 'carp'\n",
    "\n",
    "ncs_carp = make_nclist(start,end,ftype,verbose)\n",
    "\n",
    "ftype = 'grid'\n",
    "ncs_grid = make_nclist(start,end,ftype,verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I'm making a dictionary to store data for top 3m for all stations for any given day\n",
    "\n",
    "    It's another dictionary, PLACES_withdat\n",
    "    I initialized it with numpy arrays of zeroes for the 4 variables I need .\n",
    "    each array is of size 3, for 3 depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PLACES_withdat = {\n",
    "'Deep Bay': {\n",
    "'NEMO grid ji': (599, 126),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "\n",
    "},\n",
    "    \n",
    "'Southern Baynes': {\n",
    "'NEMO grid ji': (602, 127),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "\n",
    "},\n",
    "\n",
    "'Northern Baynes': {\n",
    "'NEMO grid ji': (646, 127),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "'Fanny Bay': {\n",
    "'NEMO grid ji': (614, 120),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "'Maple Bay': {\n",
    "'NEMO grid ji': (392, 213),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "'Salt Spring': {\n",
    "'NEMO grid ji': (386, 218),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "'Nanoose Bay': {\n",
    "'NEMO grid ji': (517, 190),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "'Lasqueti Island': {\n",
    "'NEMO grid ji': (586, 195),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "'Main SoG': {\n",
    "'NEMO grid ji': (450, 253),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "'Cortes/Marina': {\n",
    "'NEMO grid ji': (732, 157),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "'Lund/Desolation Sound': {\n",
    "'NEMO grid ji': (702, 187),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "'Mouth of Okeover': {\n",
    "'NEMO grid ji': (726, 192),\n",
    "    'T_timeseries':np.zeros([3]),\n",
    "    'S_timeseries':np.zeros([3]),\n",
    "    'DIC_timeseries':np.zeros([3]),\n",
    "    'TA_timeseries':np.zeros([3]),\n",
    "},\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TJSJ Now let's open the carp and grid dataset for the first day of the year\n",
    "\n",
    "    first day of year is jan 1st 2013, 0th index in our array\n",
    "    let's just print out our filename and what's in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename is: \n",
      "/results/SalishSea/hindcast.201905/01jan13/SalishSea_1h_20130101_20130101_carp_T.nc\n",
      "\n",
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    name: SalishSea_1h_20130101_20130105\n",
      "    description: auxilary variables\n",
      "    title: auxilary variables\n",
      "    Conventions: CF-1.6\n",
      "    timeStamp: 2019-Sep-14 19:05:50 GMT\n",
      "    uuid: a850fe2d-5636-424e-b1a7-fb49f3240f05\n",
      "    dimensions(sizes): axis_nbounds(2), x(398), y(898), nvertex(4), deptht(40), time_counter(24)\n",
      "    variables(dimensions): float32 \u001b[4mnav_lat\u001b[0m(y,x), float32 \u001b[4mnav_lon\u001b[0m(y,x), float32 \u001b[4mbounds_lon\u001b[0m(y,x,nvertex), float32 \u001b[4mbounds_lat\u001b[0m(y,x,nvertex), float32 \u001b[4marea\u001b[0m(y,x), float32 \u001b[4mdeptht\u001b[0m(deptht), float32 \u001b[4mdeptht_bounds\u001b[0m(deptht,axis_nbounds), float32 \u001b[4mPAR\u001b[0m(time_counter,deptht,y,x), float64 \u001b[4mtime_centered\u001b[0m(time_counter), float64 \u001b[4mtime_centered_bounds\u001b[0m(time_counter,axis_nbounds), float64 \u001b[4mtime_counter\u001b[0m(time_counter), float64 \u001b[4mtime_counter_bounds\u001b[0m(time_counter,axis_nbounds), float32 \u001b[4msigma_theta\u001b[0m(time_counter,deptht,y,x), float32 \u001b[4me3t\u001b[0m(time_counter,deptht,y,x), float32 \u001b[4mFraser_tracer\u001b[0m(time_counter,deptht,y,x), float32 \u001b[4mdissolved_inorganic_carbon\u001b[0m(time_counter,deptht,y,x), float32 \u001b[4mtotal_alkalinity\u001b[0m(time_counter,deptht,y,x), float32 \u001b[4mdissolved_oxygen\u001b[0m(time_counter,deptht,y,x)\n",
      "    groups: \n",
      "\n",
      "filename is: \n",
      "/results/SalishSea/hindcast.201905/01jan13/SalishSea_1h_20130101_20130101_grid_T.nc\n",
      "\n",
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    name: SalishSea_1h_20130101_20130105\n",
      "    description: ocean T grid variables\n",
      "    title: ocean T grid variables\n",
      "    Conventions: CF-1.6\n",
      "    timeStamp: 2019-Sep-14 19:05:50 GMT\n",
      "    uuid: 65070fb9-03a2-4fc4-89e8-9e8f3cb01f12\n",
      "    dimensions(sizes): axis_nbounds(2), x(398), y(898), nvertex(4), deptht(40), time_counter(24)\n",
      "    variables(dimensions): float32 \u001b[4mnav_lat\u001b[0m(y,x), float32 \u001b[4mnav_lon\u001b[0m(y,x), float32 \u001b[4mbounds_lon\u001b[0m(y,x,nvertex), float32 \u001b[4mbounds_lat\u001b[0m(y,x,nvertex), float32 \u001b[4marea\u001b[0m(y,x), float32 \u001b[4mdeptht\u001b[0m(deptht), float32 \u001b[4mdeptht_bounds\u001b[0m(deptht,axis_nbounds), float32 \u001b[4msossheig\u001b[0m(time_counter,y,x), float64 \u001b[4mtime_centered\u001b[0m(time_counter), float64 \u001b[4mtime_centered_bounds\u001b[0m(time_counter,axis_nbounds), float64 \u001b[4mtime_counter\u001b[0m(time_counter), float64 \u001b[4mtime_counter_bounds\u001b[0m(time_counter,axis_nbounds), float32 \u001b[4mvotemper\u001b[0m(time_counter,deptht,y,x), float32 \u001b[4mvosaline\u001b[0m(time_counter,deptht,y,x)\n",
      "    groups: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "day = 0\n",
    "#TJSJ carbon data\n",
    "print('filename is: ')\n",
    "print(ncs_carp[0])\n",
    "print()\n",
    "t_carp = nc.Dataset(ncs_carp[0])\n",
    "#print what's in the dataset\n",
    "print(t_carp)\n",
    "#TJSJ grid data\n",
    "print('filename is: ')\n",
    "print(ncs_grid[0])\n",
    "print()\n",
    "t_grid = nc.Dataset(ncs_grid[0])\n",
    "#print what's in the dataset\n",
    "print(t_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TJSJ now let's take out the data, check its size, and then average it to daily from hourly\n",
    "\n",
    "   variable size is (24, 40, 898, 398): 24 hours, 40 depths, 898, 398\n",
    "   daily averaged variable size is (40, 898, 398)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of SAL variable\n",
      "(24, 40, 898, 398)\n",
      "shape of SAL_daily\n",
      "(40, 898, 398)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SAL = t_grid['vosaline']\n",
    "TEMP = t_grid['votemper']\n",
    "DIC = t_carp['dissolved_inorganic_carbon']\n",
    "TA = t_carp['total_alkalinity']\n",
    "\n",
    "print('shape of SAL variable')\n",
    "print(np.shape(SAL))\n",
    "\n",
    "SAL_daily = np.mean(SAL,axis=0)\n",
    "TEMP_daily = np.mean(TEMP,axis=0)\n",
    "DIC_daily = np.mean(DIC,axis=0)\n",
    "TA_daily = np.mean(TA,axis=0)\n",
    "print('shape of SAL_daily')\n",
    "print(np.shape(SAL_daily))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TJSJ loop through list of stations, storing T,S,DIC,TA in the dictionary we made\n",
    "\n",
    "    as a check, I'm printing out the DIC values for the stations we're looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station is: Deep Bay. j is: 599, i is: 126\n",
      "DIC values (top 3m) are: [1727.4579 1848.1748 1950.4027]\n",
      "TA values (top 3m) are: [1775.049  1902.7909 2010.8522]\n",
      "\n",
      "Station is: Southern Baynes. j is: 602, i is: 127\n",
      "DIC values (top 3m) are: [1794.9349 1929.7383 1968.4567]\n",
      "TA values (top 3m) are: [1847.9092 1989.7203 2029.9442]\n",
      "\n",
      "Station is: Northern Baynes. j is: 646, i is: 127\n",
      "DIC values (top 3m) are: [1745.107  1972.7373 1995.0317]\n",
      "TA values (top 3m) are: [1789.4132 2029.889  2053.1213]\n",
      "\n",
      "Station is: Fanny Bay. j is: 614, i is: 120\n",
      "DIC values (top 3m) are: [1676.3019 1860.8667 1967.1818]\n",
      "TA values (top 3m) are: [1720.3026 1919.1771 2031.0568]\n",
      "\n",
      "Station is: Maple Bay. j is: 392, i is: 213\n",
      "DIC values (top 3m) are: [1897.8878 1920.7955 1934.87  ]\n",
      "TA values (top 3m) are: [1966.7697 1990.2909 2004.5367]\n",
      "\n",
      "Station is: Salt Spring. j is: 386, i is: 218\n",
      "DIC values (top 3m) are: [1933.167  1964.1224 1973.9298]\n",
      "TA values (top 3m) are: [2002.1011 2032.6025 2041.6934]\n",
      "\n",
      "Station is: Nanoose Bay. j is: 517, i is: 190\n",
      "DIC values (top 3m) are: [1834.9097 1980.5913 1996.7657]\n",
      "TA values (top 3m) are: [1892.0746 2043.6973 2059.953 ]\n",
      "\n",
      "Station is: Lasqueti Island. j is: 586, i is: 195\n",
      "DIC values (top 3m) are: [1967.3525 1969.336  1973.4614]\n",
      "TA values (top 3m) are: [2040.8251 2042.8312 2046.9183]\n",
      "\n",
      "Station is: Main SoG. j is: 450, i is: 253\n",
      "DIC values (top 3m) are: [1903.4476 1919.8578 1939.3693]\n",
      "TA values (top 3m) are: [1974.4982 1992.2002 2012.2897]\n",
      "\n",
      "Station is: Cortes/Marina. j is: 732, i is: 157\n",
      "DIC values (top 3m) are: [1854.5187 1868.0392 1876.4302]\n",
      "TA values (top 3m) are: [1922.8066 1936.9125 1945.7026]\n",
      "\n",
      "Station is: Lund/Desolation Sound. j is: 702, i is: 187\n",
      "DIC values (top 3m) are: [1734.459  1787.1768 1822.5568]\n",
      "TA values (top 3m) are: [1797.3707 1851.8843 1887.944 ]\n",
      "\n",
      "Station is: Mouth of Okeover. j is: 726, i is: 192\n",
      "DIC values (top 3m) are: [1642.2277 1746.4619 1782.7845]\n",
      "TA values (top 3m) are: [1695.3937 1808.5709 1845.7509]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(list_places)):\n",
    "    t_place = list_places[i]\n",
    "    t_j_index = PLACES[t_place]['NEMO grid ji'][0]\n",
    "    t_i_index = PLACES[t_place]['NEMO grid ji'][1]\n",
    "    print('Station is: ' + t_place + '. j is: '+str(t_j_index)+', i is: '+str(t_i_index))\n",
    "    \n",
    "    t_DIC = DIC_daily[0:3,t_j_index,t_i_index]\n",
    "    t_TA = TA_daily[0:3,t_j_index,t_i_index]\n",
    "    t_SAL = SAL_daily[0:3,t_j_index,t_i_index]\n",
    "    t_TEMP = TEMP_daily[0:3,t_j_index,t_i_index]\n",
    "    print('DIC values (top 3m) are: '+str(t_DIC))\n",
    "    print('TA values (top 3m) are: '+str(t_TA))\n",
    "    print()\n",
    "    \n",
    "    PLACES_withdat[t_place]['T_timeseries'] = t_TEMP\n",
    "    PLACES_withdat[t_place]['S_timeseries'] = t_SAL\n",
    "    PLACES_withdat[t_place]['DIC_timeseries'] = t_DIC\n",
    "    PLACES_withdat[t_place]['TA_timeseries'] = t_TA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TJSJ now I'll name an nc filename and store the data in it\n",
    "\n",
    "    is there a better/more elegant way to \n",
    "    write the code that writes the data to the .nc file?\n",
    "    almost definitely\n",
    "    don't know what it is though \n",
    "    and can't be bothered googling stackoverflow for a long time\n",
    "    to try to figure out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our new .nc file will be named:\n",
      "EVIE_PLACES_TEST3_01jan13.nc\n"
     ]
    }
   ],
   "source": [
    "#TJSJ\n",
    "str_name = 'EVIE_PLACES_TEST3_'\n",
    "ymd = '01jan13'\n",
    "t_str = str_name + ymd + '.nc'\n",
    "print('our new .nc file will be named:')\n",
    "print(t_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f = nc.Dataset(t_str, 'w')\n",
    "g = f.createGroup('Deep Bay')\n",
    "g.createDimension('depths', 3)\n",
    "g.createDimension('days',365)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Deep Bay']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Deep Bay']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Deep Bay']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Deep Bay']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Southern Baynes')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Southern Baynes']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Southern Baynes']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Southern Baynes']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Southern Baynes']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Northern Baynes')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Northern Baynes']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Northern Baynes']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Northern Baynes']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Northern Baynes']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Fanny Bay')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Fanny Bay']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Fanny Bay']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Fanny Bay']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Fanny Bay']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Maple Bay')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Maple Bay']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Maple Bay']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Maple Bay']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Maple Bay']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Salt Spring')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Salt Spring']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Salt Spring']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Salt Spring']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Salt Spring']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Nanoose Bay')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Nanoose Bay']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Nanoose Bay']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Nanoose Bay']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Nanoose Bay']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Lasqueti Island')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Lasqueti Island']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Lasqueti Island']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Lasqueti Island']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Lasqueti Island']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Main SoG')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Main SoG']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Main SoG']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Main SoG']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Main SoG']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Cortes/Marina')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Cortes/Marina']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Cortes/Marina']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Cortes/Marina']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Cortes/Marina']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Lund/Desolation Sound')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Lund/Desolation Sound']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Lund/Desolation Sound']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Lund/Desolation Sound']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Lund/Desolation Sound']['TA_timeseries'][:]\n",
    "\n",
    "g = f.createGroup('Mouth of Okeover')\n",
    "g.createDimension('depths', 3)\n",
    "ts = g.createVariable('T_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Mouth of Okeover']['T_timeseries'][:]\n",
    "ts = g.createVariable('S_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Mouth of Okeover']['S_timeseries'][:]\n",
    "ts = g.createVariable('DIC_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Mouth of Okeover']['DIC_timeseries'][:]\n",
    "ts = g.createVariable('TA_timeseries','f4',('depths'))\n",
    "ts[:] = PLACES_withdat['Mouth of Okeover']['TA_timeseries'][:]\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TJSJ let's open the .nc file we just wrote and make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Group'>\n",
      "group /Deep Bay:\n",
      "    dimensions(sizes): depths(3), days(365)\n",
      "    variables(dimensions): float32 \u001b[4mT_timeseries\u001b[0m(depths), float32 \u001b[4mS_timeseries\u001b[0m(depths), float32 \u001b[4mDIC_timeseries\u001b[0m(depths), float32 \u001b[4mTA_timeseries\u001b[0m(depths)\n",
      "    groups: \n",
      "\n",
      "[1727.4579 1848.1748 1950.4027]\n",
      "[1775.049  1902.7909 2010.8522]\n"
     ]
    }
   ],
   "source": [
    "w = nc.Dataset(t_str)\n",
    "print(w['Deep Bay'])\n",
    "print(w['Deep Bay']['DIC_timeseries'][:])\n",
    "print(w['Deep Bay']['TA_timeseries'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
